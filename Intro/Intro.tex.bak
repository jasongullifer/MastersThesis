%SourceDoc ../YourName-Dissertation.tex
\chapter{Introduction}
%\section{Introduction}
      Anyone who has had exposure to a second language (L2), even if for a brief amount of time, has likely noticed that the L2 may share some of the same words and lexical features with the first language (L1). For instance, in both Dutch and English the word hotel refers to an establishment that provides lodging for guests. The word hotel is a cognate in Dutch and English: a translation equivalent that shares similar lexical form in both languages (i.e., orthography and phonology). Words in two languages may also overlap in lexical form without sharing the same meaning; these are called false cognates or homographs. For example, the English translation of the Dutch word room is cream. An example with more sinister consequences is the German word for poison: Gift. While observations such as these may seem amusing when first learning a new language, it would seem absurd to assume that a proficient German-English bilingual would think of poison upon hearing the English word gift. However, this may not be far from the truth, as recent evidence suggests that bilinguals may not be able to function as the sum of two monolinguals (Grosjean, 1989).\parencite{Duyck2007}

      While listening to spoken language, reading written text, or preparing words and sentences to be spoken, the bilingual appears to have both languages active momentarily before the intended language is selected (e.g., Dijkstra, 2005, Kroll, Sumutka, \& Schwartz, 2005; Marian \& Spivey, 2003). Research initially suggested that the dominant L1 was more likely to intrude when bilinguals were performing a task in their weaker L2 (e.g., Dijkstra, Grainger, \& van Heuven, 1999; Dijkstra, van Jaarsveld, \& ten Brinke, 1998). More recent findings demonstrate that bilinguals' experience with an L2 or even L3 can also influence their performance when completing a task in the more dominant L1 (e.g., van Hell \& Dijkstra, 2002). This parallel activation of the two languages has been referred to as \textit{language non-selectivity} and it manifests itself as a cross-language interaction when bilinguals encounter words that are ambiguous across the two languages (e.g., hotel or room when read by a bilingual who speaks Dutch and English). The observed interactions are not categorical but can be modulated by the relative degree of cross-language overlap (e.g., Schwartz, Kroll, \& Diaz, 2007). 

      The observation of parallel activation of two languages when only one is required has had profound consequences for our understanding of the bilingual experience. Critically, bilinguals are able to effectively juggle alternatives in both languages simultaneously without producing random errors of language in spontaneous speech (Poulisse, 1994). At the same time, it is a well-known fact that many bilinguals are able to code-switch with each other, changing languages effortlessly even within a sentence. These switches are often strategic in nature and follow a set of grammatical and social rules as opposed to being random slips of language (e.g., Myers-Scotton, 2002). The ability to negotiate parallel activation of the two languages, to exploit its presence during code-switching, and to avoid its consequences in single language discourse suggest that bilinguals develop the ability to exert control over language processes (e.g., Green, 1998). In fact, bilingualism appears to confer a set of measureable benefits in the realm of executive function (e.g., Bialystok, 2005). The hypothesis is that a life of learning to juggle the parallel activation of the multiple languages creates cognitive expertise that generalizes beyond language to attentional control. 
 
      As exciting as the cognitive consequences of language non-selectivity may be, under ordinary circumstances, bilinguals must eventually select one language to read, hear, or speak. Most of the early studies that have found evidence for language non-selectivity were been conducted outside of any meaningful context; participants were required to make decisions about or name single words aloud. Without additional context, lexical ambiguity is at its peak. Thus, parallel activation may be a byproduct of the way in which word recognition experiments are performed rather than a feature of ordinary bilingual language processing. To determine whether this is the case, a set of recent studies has asked whether the evidence for language non-selectivity can also be observed within sentence context (Chambers \& Cooke, 2009; Duyck, van Assche, Drieghe, \& Hartsuiker 2007; Libben \& Titone, 2009; Schwartz \& Kroll, 2006; van Assche, Duyck, Hartsuiker, \& Diependaele 2009; van Hell \& De Groot, 2008). Using a range of experimental methods, these studies have converged upon the conclusion that sentence context in and of itself is not sufficient to constrain lexical access to one language. This is true when bilinguals listen to spoken words in the L2 (e.g., Chambers \& Cooke, 2009) or read the L2 (Duyck et al., 2007; Libben \& Titone, 2009; Schwartz \& Kroll, 2006; van Hell \& de Groot, 2008). Parallel activation of the alternative language has also been observed when bilinguals read words in L1 sentence contexts (van Assche et al., 2009). If sentence context itself cannot constrain language selection, then other factors must be doing so.

      In order to begin to understand the variables that allow bilinguals to restrict language selection in sentence context, the concept of the sentence must be deconstructed. In typical language use, a sentence contains words grouped together in order to express a meaning. Two important and distinct aspects of this characterization are the meaning of the sentence (the semantics) and the rules that govern the ordering of words in the sentence (the syntax). Both the semantics and the syntax can impose restrictions or constraints on upcoming words in a sentence. For example, the sentence John kept his gym clothes in the locker imposes a semantic restriction; the semantic context of the sentence (established by gym clothes) makes the word locker more predictable. This sentence is said to have a high semantic constraint. In such a highly constrained context, monolingual readers experience facilitated processing of the word target word locker (e.g., Schwanenflugel \& LaCount, 1988; Stanovich \& West, 1983).

      For bilinguals, semantic constraints may play a role in assisting with the selection of the intended language. Similar to the example above, the sentence When the composer entered the hall, he sat at the bench and began to play the piano is highly semantically constrained. The context established by the meanings of the words composer, hall, and bench all converge on an interpretation of a music hall and therefore the word piano is predictable. Compare this to a sentence with a low semantic constraint: When we entered the dining hall we saw the piano in the corner of the room. Here the word piano could be nearly any other noun (e.g., cat, table, lamp). Critically in this example, the word piano is language ambiguous between English and Spanish; piano is a cognate. When bilinguals read sentences with a low semantic constraint, they continue to activate both language interpretations of the cognate as if they were processing the word out of context. In contrast, when English-Spanish bilinguals read sentences that are highly constrained semantically, they appear to access the cognate as if it were a word only in the language of the sentence (e.g., Libben \& Titone, 2009; Schwartz \& Kroll, 2006; van Hell \& De Groot, 2008; but see van Assche, 2009).1 That is, word recognition appears to be language-selective when ambiguous words appear in sentences that are semantically constrained.

      The majority of evidence supports the notion that semantics are completely or partially shared across the bilingual's languages (e.g., Costa, Miozzo, \& Caramazza, 1999; Fox, 1996; Kroll \& Scholl, 1992; Kroll, 1993; Potter, So, von Eckhardt, \& Feldman, 1984; Schwanenflugel \& Rey, 1986; Smith 1991; Snodgrass, 1984). Thus, there is no reason a priori that semantics itself should provide a solid language that would facilitate language selection. How, then, could a highly biased context eliminate parallel activity of the unintended language? Recall the high constraint sentence When the composer entered the hall, he sat at the bench and began to play the piano. By the time the reader gets to the word piano, the set of lexical candidates has been heavily restricted and the potential candidates (e.g., musical instruments) have received activation. For a bilingual, this activation, in turn, may allow early selection of the intended language causing the reader to ignore the existence of piano in the unused language. However, the role of the semantics in curtailing parallel activity of the unintended language is indirect in nature and, hence, the effect may be reliable only under some circumnstances. There must be other factors at work to enable bilinguals to select the intended language. After all, not all sentences are highly biased toward a specific meaning. What, in those cases, allows a bilingual to speak the target language?

      The past studies on reading language ambiguous words in context have examined the role of semantic constraints. However, the semantics are not the only source of restrictions on upcoming words; the syntactic structure of the sentence also provides constraints. These restrictions become apparent when they are violated causing readers to slow down dramatically. Even more interesting is the fact that languages differ in the way that syntactic coherence is achieved (in contrast to the semantics which are largely shared). In extreme cases, sentence word order can differ completely across languages. To illustrate, whereas English uses a Subject-Verb-Object (e.g., The child sees the house) word ordering, Fijian uses a Verb-Object-Subject ordering (e.g., Sees the house the child). What are the consequences for a bilingual speaker of English and Fijian of having separate word orders? This difference might provide reliable language cues to allow bilinguals to selectively access words in sentence context. 

      The case of Fijian and English word order is an extreme condition but even languages that are typologically similar can have differences in the syntax. Take, for example, the Spanish sentence and English sentences in \ref{lasmonjas}.

\ex.\label{lasmonjas}      Las monjas le llevaron las mantas que [pro] habían bordado a la directora del orfanato.\\
      The nuns took the quilts that they had embroidered to the director of the orphanage

      Two grammatical features of the Spanish sentence differ from the English translation. First, the Spanish sentence contains the proclitic le that redundantly refers to the object of the verb took (la directora). (A proclitic works similarly to a pronoun (e.g., he, she, it) except that the proclitic refers to an entity occurring later in the sentence and this reference may span quite a long distance). Second, the subject of the relative clause \textit{habían bordado} (they had embroidered) is phonologically null in Spanish (indicated in the example with the null pronoun pro). Unlike Spanish, English does not use proclitics nor is it typical for English to drop the subject of a relative clause. These aspects of grammar that differ between languages can be considered language-specific syntactic constructions.

      If the presence of language-specific syntactic constructions in a sentence allow the reader or listener \textit{zoom in} to the target language (e.g., Elston-Güttler, Gunter, \& Kotz, 2005), then lexical access should proceed in a selective manner. That is, cognates embedded in sentences with language-specific syntax (and with a low semantic bias) should be accessed as if they are a word only in the language of that sentence (i.e., a noncognate word). To my knowledge, this claim has not yet been tested directly in previous studies.

      The purpose of the current proposal is to further explore the mechanisms that allow bilinguals to control selection of the intended language by having participants read words either within or outside of a sentence context. The first goal is to replicate previous studies by demonstrating that both languages are active when Spanish-English bilinguals name single words. Those words will then be placed in sentence context to test the hypothesis that syntactic constructions specific to only one of a bilingual's languages will bias bilinguals toward the selection of the intended language. This will be carried out by asking participants to name words that are presented in sentences either with or without language-specific syntactic constraints. Critically, all sentences in the experiment will be of low semantic constraint to maximize parallel activation of both languages. In the section that follows I review the evidence for language non-selectivity in out-of-context word recognition tasks and within sentence context. Following the review, I describe the design of the proposed study. 

\section{Out of context word recognition}\label{Intro::OOC}
      Evidence for non-selectivity is seen reliably when participants are asked to read, hear, or produce single words in one of the two languages. Many of these studies exploit the presence of ambiguities in the written form and pronunciation of words in the bilingual's two languages to demonstrate that bilinguals, but not monolinguals, are sensitive to the properties of the language not in use. Although evidence for non-selectivity has been found using a wide array of stimuli (e.g., cognates, homographs, and words with many cross-language neighbors), the cognate effect, that cognates are typically recognized fast than noncognate controls, has been the most robust (e.g., Dijkstra, van Jaarsveld, \& ten Brinke, 1998). Influence from the unintended language is seen regardless of whether the L2 or the L1 is in use (Dijkstra et al. 1998; Grainger \& Dijkstra, 1992; Grainger \& Dijkstra 1999; van Hell \& Dijkstra, 2002). Furthermore, both phonological and orthographical representations remain active (e.g., Marian \& Spivey, 2003; Jared \& Kroll, 2001) regardless of script differences between the two languages (e.g., Gollan, Forster, \& Frost, 1997). 

      If a bilingual can selectively access one of her two languages, then words that are language ambiguous should be recognized no differently than words that are language non-ambiguous. For this reason, language ambiguous words have been the focus of many studies investigating lexical access in bilingual speakers. Two types of language ambiguous words, cognates and homographs, are of particular interest to psycholinguists because they are the words that are the most ambiguous between languages. Sharing nearly complete overlap between two languages (similar orthography, phonology, and meaning), a cognate is the prime example of a language ambiguous word. Homographs are also highly language ambiguous, except that they do not share meaning. Therefore, it is reasonable to predict that cognates may allow for greater parallel activation of the two languages.
Dijkstra, Van Jaarsveld, and Ten Brinke explored the role of stimulus list composition and task instructions on effects of parallel activation. They showed that cognates robustly allow for measurement of parallel activation of languages compared to homographs. In three separate experiments, they asked Dutch-English participants to make lexical decisions to single words. In the first two experiments, participants were told to make a ``yes'' decision for English, their L2. The stimuli consisted of the stimuli consisted of cognates, homographs, and English noncognate words. These stimuli were supplemented with a set of Dutch words in the second experiment. The Dutch words required a ``no'' response. In the third experiment, participants were told to respond ``yes'' to both Dutch and English words. In all three experiments, Dijkstra et al. found a facilititory cognate effect. However, the nature of the homograph effect depended on the instructions of the other stimuli and instructions of the task. 

In experiment 1, which included no Dutch distractor words, homographs were not recognized differently from control words, but when Dutch words were included (experiment 2), an inhibitory homograph effect emerged.  The inclusion of Dutch words raised activation of Dutch, a language requiring a ``no'' response.  Thus participants had to be more cautious in their responses to words that seemed Dutch (including homographs). When participants were allowed to respond to either language in the third experiment, this caution was no longer necessary and participants could again rely on cross-language overlap especially at the level of orthography, causing homographs, like cognates, to become facilitated. Critically, there was always a cognate effect no matter the instructions or surrounding materials, suggesting that the cognate effect is relatively robust compared to the homograph effect. [[I feel like there's also something about this experiment that suggests that semantics isn't always activated during lexical decision, but I can't quite elucidate that thought. Maybe because there was no homograph effect in exp 1, but it could be explained only by resting activation, which gets more support from the lack of homograph effects in other studies like Schwartz and Kroll which used naming and would be the reason we're advocating naming in the first place because it must activate semantics. Also, it could be happening in exp3 that semantics is bypassed... otherwise you might still suspect an inhibitory effect.]]

      Dijkstra et al. (1998) provided evidence for the assumption that cognates may strongly activate both languages in bilingual speakers. In a series of experiments using the lexical decision task (LDT), Dijkstra et al. investigated the recognition of cognates and homographs by Dutch-English bilinguals. In a typical LDT, and the type that Dijkstra et al. use in two of their experiments, participants are asked to decide if target stimuli, displayed one at a time, are licit words in the language specified by the experimenter. Target stimuli consist of real words and nonword distractors. Nonword distractors, also called pseudo-words, follow the phonotactic properties of the language of the task. In an example of a typical LDT trial, a Dutch-English participant may have to decide if the word \textit{room} (a homograph) is a word in English (a so-called English lexical decision). Dijkstra et al. also employed what they call a \textit{general lexical decision task} in which participants respond \textit{yes} for a word that is in either of their two languages. In the first experiment, they included both cognates and noncognates in an English LDT. They found that participants were faster to recognize cognate words compared to non-ambiguous controls; this effect is commonly referred to as \textit{the cognate effect}. Furthermore, they found no difference in the recognition of homographs compared the controls. Two later experiments, in which they varied the task and materials, revealed a homograph effect. Importantly, the homograph effect emerged only after L1 words were included. For the purpose of the present study, two critical findings can be gleaned from Dijkstra et al. First, the cognate effect was witnessed regardless of the fact that there were no L1 words embedded in the stimuli. Conversely, the homograph effect needed to be given a boost in activation of the L1 in order to surface. The fact that a boost was necessary for the homographs to affect recognition is indicative that the homograph effect is weaker than the cognate effect. The second critical finding in this study is that L2 speakers of English are activating their L1 when they are participating in a task that is solely conducted in their L2. 

      The evidence for parallel activation of the stronger L1 when using the weaker L2 may not be surprising. In fact, it seems obvious that a more dominant system would impede on a less dominant one. However, parallel activation is not simply a function of language dominance or proficiency. Tasks where bilinguals use only the more dominant L1 have yielded a striking and counterintuitive result: knowledge of a second language influences how bilinguals use their first language. Van Hell and Dijkstra (2002) asked highly proficient Dutch-English-French trilinguals to participate in a Dutch lexical decision task. The participants were recruited in the Dutch language and were given no knowledge that either French or English were in any way involved in the study. The LDT contained Dutch words in each of the following categories: cognates between Dutch and French but not English (e.g., Dutch: meuble [French: meuble; English: piece of furniture]), cognates between Dutch and English but not French (e.g., Dutch: bakker [English: baker; French: boulanger]), and non-cognate controls (Dutch: fiets [French: vélo; English: bike]). The stimuli also contained a set of Dutch pseudo-words. Pseudo-words, like the nonwords above, follow the phonotactic regularities of a language but have no meaning (e.g., raponse in Dutch or wug in English). The results of the study showed that participants were reliably faster to judge Dutch-English and Dutch-French cognates as words than non-cognate controls. The participants believed that this task was conducted only in Dutch and had no reason to suspect that English or French was in any way involved. Yet, they could not prevent the activation of their L2 and L3 when they are reading the cognate words. This result demonstrates that the bilingual language processing system functions in a non-selective manner even when recognizing words in the stronger L1.

      Thus far, we have seen evidence for parallel activation when bilinguals process language-ambiguous words in either their L1 or L2. A question that now arises is how deep this parallel activation runs. In the two studies above, bilinguals were performing LDTs that involved reading words. Thus, we have learned that the orthography of both languages is active. Although cognates are said to overlap in both orthography and phonology, there can actually be subtle differences between them across languages. For example, while base, a cognate, is written identically in English and Spanish, the pronunciations are noticeably different. Thus it's reasonable to suspect that the phonology of the cognates plays a role in their recognition as well. 

      Schwartz et al. (2007) looked at the role of phonology in more detail. In this study, instead of the LDT, participants were asked complete a word naming task. Words appear one at a time on a computer screen and they are asked to name the word aloud; the reaction time (RT) to the onset of articulation is measured. The word naming task is similar to the LDT in that it measures recognition of the word; however, in addition, the naming task requires participants to select a single word and access its phonology. Schwartz and Kroll embedded Spanish cognates and their English translations into two language blocks. Each cognate was matched to a noncognates control word. The cognates varied orthogonally in the degree of phonological (P) and orthographic (O) overlap (+O+P: piano-piano, +O-P: base-base, -O+P: tren-train, -O-P: marca-mark). A group of English-Spanish bilinguals were asked to name words in each of the language blocks. The critical result observed was that cognates that overlapped in orthography but not phonology were named more slowly than cognates with converging phonology and orthography (+O -P > +O +P). This difference was observed when participants were naming word in both their L1 and L2. These data suggest that bilinguals activate phonological representations of word in the unintended language via orthographic codes (see also Jared \& Kroll 2001). Furthermore, these data provide evidence that cross-language interactions can be modulated by the degree of overlap between the two languages. 

      The evidence for language non selectivity is not restricted to visual stimuli. Bilinguals activate the phonology of the unintended language when they hear either of their languages (e.g., Marian \& Spivey, 2003). Furthermore, this parallel activation of phonology and orthography occurs regardless of script differences between the bilingual's two languages (e.g., Gollan et al., 1997). Overall, bilinguals seem fundamentally open to cross-language similarity. Yet, there must be a control mechanism that allows them to eventually select a language since bilinguals can successfully speak or listen to one language alone. 


\section{In context word recognition}
The previous studies reviewed thus far have been conducted asking participants to recognize single words. However, reading rarely involves the recognition of single words without any relation to one another. Instead, words are part of sentences which themselves are part of paragraphs. Likewise in speech, words are components of sentences in a discourse. Each level contributes a new layer of context. Because language use typically occurs in a rich context, out of context experiments provide an artificial environment for reading. For bilinguals, the ambiguity of language membership for each word may also be heightened. This may allow for parallel activation of both languages that would not otherwise exist in an environment with context. More recent research has sought to explore the role of sentence context on bilingual word recognition, investigating whether both language are still activated in parallel when a sentence is in one language alone. Only a handful of studies have examined this question, and the results have been interpreted in a variety of ways. [[Maybe need a sentence that sets up that there are two issues here, 2 nonselectivit, and one whether it can be reduced]] 

The fundamental question addressed by this most recent set of studies is whether evidence for nonselectivity can be seen in sentence context. Thus far, every study examining this question has come to the same conclusion: parallel activation does persist even when there is meaningful, unilingual context. This result is amazingly counterintuitive; an English word surrounded by other words that are unambiguously English activates words in another language.  It would seem that language nonselectivity is a fundamental property of the bilingual word recognition system and is does not arise as the result of experimental contexts. 

While the results of all studies converge on the fact that word recognition in sentence context is nonselective, there is disagreement on the issue of whether certain types of linguistic contexts can provide bilinguals a cue to the language of the sentence and work to lessen or even eliminate parallel activation of the unintended language. A set of studies has shown that semantic constraints[[explain]] can, at least sometimes, allow the system to function in a selective manner. The present study attempts to address this debate by exmaining another type of context that could work similarly to semantic constraints. 

%% The previous work reviewed thus far has been conducted asking participants to recognize single words.  However, reading rarely involves the recognition of single words without any relation to one another. Instead, words are part of sentences which themselves are part of paragraphs with each level contributing a new layer of context. Likewise in speech, words are components of sentences in a discourse. Language use typically occurs in a rich context. Out of context experiments may thus provide an artificial environment devoid of context, where the ambiguity of language membership of each word is heightened allowing parallel activation of both languages. More recent work has sought to explore the role of sentence context on bilingual word recognition, investigating whether both language are still activated in parallel when a sentence is in one language alone. Only a handful of studies have explored this question, and the results have been interpreted in a variety of ways. 

%Some researchers (e.g., Heridia at Psychonomics, and likely Duyck as well) would caution that the results of these in context studies are unclear, painting the picture for each study demonstrating nonselective access in context (e.g., Ducyk 2007, arguably Libben and Titone, Van Assche 2010, Van Assche 2009) there is a study that shows selective access (Schwartz and Kroll, Van Hell, arguably Libben and Titone, Chambers and Cook). However, if we are only allowed to decide that word recognition in sentence context is ``selective'' or ``nonselective'', then the results will appear mixed. If we instead allow language selection to occur on a continuum that ranges from nonselective to selective, then the results become clearer. Under this light, evidence from a variety of converging methodologies confirms that the bilingual word recognition system is, for the most part, nonselective in nature, though there are certain contexts (e.g., highly biased semantic constraints; Schwartz and Kroll, Chambers and Cooke, Libben and Titone) that allow the system to become selective. The selectivity seems to take place during later stages of processing (e.g., semantic integration). Importantly, forcing the system to become selective is not easy (e.g., Van Assche 2010). What has not yet been explored is whether there are other contexts that may allow the word recognition system to select a single language, and whether there exists a context that allows for selective access from the earliest stages in processing.  

\subsection{Evidence for nonselectivity in sentence context}
There is strong evidence that bilingual word recognition in sentence context is nonselective in nature. This is seen across a variety of methodologies, for example in word naming (e.g., Schwartz and Kroll, 2006); in lexical decision and translation (e.g., Van Hell and De Groot, 2008); in eye-tracking measures of reading (e.g., Duyck et al., 2007; Libben and Titone, 2009; Van Assche et al., 2009; and Van Assche et al., 2010); and during auditory word recognition (Chambers and Cooke, 2009;). The unintended language is activated regardless of whether the less dominant L2 is in use (e.g., Chambers and Cooke, 2009; Schwartz and Kroll, 2006; Duyck et al., 2007; Van Assche et al., 2010; Libben and Titone, 2009) or the more dominant L1 is use (Van Assche et al., 2009; Schwartz and Kroll, 2006). The degree of parallel activation is shown to be a function of the amount of cross-language overlap, suggesting parallel activation is not unique to cognates. Language nonselectivity seems to be a fundamental property of bilingual word recognition system. This finding is quite counterintuitive. It means that despite being aware that a sentence is entirely written in one language, a bilingual will activate both languages. The fact that a sentence has coherant semantics and syntax cannot eliminate activation of the unintended language. However, there is evidence that if a sentence is very strongly biased towards a single interpretation, parallel activity can be modulated.  

%%%% Methodology
Researchers have used a variety of methodologies to study language nonselectivity. Each method can provide a unique look at the phenomenon, particularly in what contexts and over what timecourse it occurs. Importantly, research shows that regardless of the method used, bilingual word recognition is nonselective in nature. This cross-method comparison is important because it shows that  shows that the parallel activation is not task dependant. 

Initial evidence for parallel activation of two languages in sentence context came from behavioral tasks such as word-naming, lexical decision and translation (e.g., Schwartz and Kroll, 2006; Van Hell and De Groot, 2008). Parallel activation has also been demonstrated in auditory word recognition, suggesting that the phenomenon is not specific to written word recognition (Chambers and Cooke, 2009). While the results of these tasks show the existence of parallel activation, they cannot speak to how early in processing the lexical candidates become activated. Are words in the two languages activated from the very beginning of processing, or do they only become activated during a later stage? The answer to this question comes from evidence from eye-tracking studies. They confirm that both languages are activated in parallel from the earliest stages of processing (e.g., Duyck et al., 2007; Libben and Titone, 2009; Van Assche et al., 2009; and Van Assche et al., 2010). 

Schwartz and Kroll (2006) were among the first to investigate bilingual word recognition in sentence context. They asked bilinguals to read sentences in both languages  across separate blocks. The sentences were presented word-by-word at a rapid pace, a method known as rapid serial visual presentation (RSVP). Target cognates and control words were marked in red and remained on the display until the participant spoke the name of the word. The time to begin speaking the word was recorded by a voice trigger. Schwartz and Kroll found that cognates were named faster than their matched controls in both the L1 and the L2. Cognate facilitation suggests that a sentence context is not enough to modulate language nonselectivity; bilinguals activated the lexical representations in both languages while in their native and second languages.

Van Hell and De Groot (2008) replicated the results of Schwartz and Kroll (2006) using a different set of behavioral tasks and a different set of languages: Dutch and English. They asked Dutch-English bilinguals either to perform lexical decisions or to name the translations of target words (cognates or controls) embedded in sentences. Again, reaction times to translate or to perform a lexical decision were faster for cognates than for controls, suggesting that bilinguals activated both languages despite target words being embedded in a single language context. Parallel activation in a translation task is not so surprising. The task instructions demand that both languages are in use and likely to be activated by the participants. In contrast, a lexical decision task does not require participants to keep both languages activated. Participants are likely unaware that knowledge of a second language is important for the experiment. This replication suggests that the parallel activation as witnessed by Schwartz and Kroll can not be attributed to properties of the task or the materials. Instead, nonselectivity  seems to be a feature of the bilingual word recognition system. 

The studies presented thus far have studied written word recognition. Chambers and Cooke (2009) show that the finding of nonselectivity is not limited to the written word. They show evidence for parallel activation during auditory word recognition. They asked English-French speakers to listen to sentences in French while viewing visual displays. During the presentation of the sentences, the participants' eye-movements were monitored, a methodology called the Visual World Paradigm (VWP). The visual displays contained the target object of the sentence along with other objects that were unrelated to the sentence. Critical displays contained one object that was a homophone to the target object. For example, a participants might hear ``Marie va d\'{e}crire la \textbf{poule} / [Marie will describe the \textbf{chicken}]'' while viewing a scene that contained a chicken and a pool (the homophone). Chambers and Cooke found that participants were more likely to fixate on distractor homophones (e.g., pool) compared to unrelated objects, suggesting that they were activating English while listening to French sentences auditorily. Something that the behavioral studies and the VWP do not tell us is what the nature of the time-course of activation is. The use of eye-tracking while participants read sentences is well suited to address this question. 

Duyck et al. (2007), Libben and Titone (2009), Van Assche et al. (2009), and Van Assche et al. (2010) all use eye-tracking to replicate the finding on nonselectivity in sentence context.  Eye-tracking allows participants to read sentences naturalistically, as one might read a sentence in a newspaper. Participants are free to read at their own pace, and are able to regress back to earlier parts of the sentence in case something is misunderstood. While reading, the eye-movement patterns of the participants are monitored. The eye-movement record allows for a detailed analysis of the time-course of lexical activation. Early reading measures, such as first fixation duration, provide information about initial lexical access, while later reading measures provide information about semantic integration. The authors exploit this sensitivity of eye-tracking to show that words are activated in both languages in parallel from the earliest stages of recognition, and that coactivation remains indefinitely, or at least until the reader moves on to the next word. Thus, parallel activation is not something that happens downstream during processing.

Duyck et al. (2007) and Van Assche et al. (2010) asked Dutch-English bilinguals read sentences in English (their L2). Like in previous studies, a portion of the sentences contained target cognate words while another portion contained lexically matched noncognate controls. The authors found that cognate words were fixated on for a shorter amount of time compared to control words. Crucially, the cognate effect is present from the earliest measures of reading (e.g., first fixation duration, which is taken to reflect initial lexical access) through later measures that reflect semantic integration. This effect has also been replicated by Van Assche et al. (2009) for Dutch-English bilinguals reading in their native language. 

Using a different language pair but the same reading paradigm, Libben and Titone (2009) show that nonselectivity in naturalistic reading is not unique to Dutch-English bilinguals. They show that Dutch-English bilinguals reading in English also show a cognate effect as well as an inhibitory effect of homograph words, another sign of nonselectivity. In agreement with Duyck et al (2007) and Van Assche et al. (2010), Libben and Titone show that the effects of cross-language activation began early and persisted through later measures of the eye-tracking record.  

The eye-tracking studies presented above show that at all points of word recognition, bilinguals have both languages activated in parallel. The results of eye-tracking experiments investigating written and auditory word recognition converge with data from a variety of behavioral studies to show that the finding of parallel activation is not dependant on the methodology being used. 

Next, we will see that, for the most part, the languages pairs that are used in these experiments the relative dominance of the languages do not change conclusions about nonselectivity in sentence context. 

%%%% L1-L2 and language pairs
An important consideration in assessing nonselectivity in sentence context is the direction of the co-activation. It is generally assumed that the L1 influences the L2 because the L2 is often the weaker language. However, evidence that the weaker  L2 can influence the stronger L1 provides a compelling case for the ubiquity of nonselectivity. While the number of studies examining parallel activation in context is limited, there is evidence that activation flows from the L1 to the L2 and vice versa. A second consideration is whether the results can be replicated across a variety of language pairs, ensuring that nonselectivity is a feature of bilingual word recognition in general, not just recognition for, say, Spanish-English bilinguals. Again, the number of in context studies is small, so more replication is necessary, but the preliminary set of studies suggests that nonselectivity is invariant to the languages spoken by the bilingual. 

The majority of evidence for nonselectivity in sentence context has been found for bilinguals reading in their L2. Duyck et al. (2007),  Van Assche et al. (2010),  Libben and Titone (2009), and  Schwartz and Kroll (2006) all tested bilinguals in English as their L2. Chambers and Cooke recruited bilinguals that use French as their L2. All resesearchers find evidence for nonselectivity in sentence context. Because, it is often assumed that the L1 has an affect on the L2, parallel activation of the L1 while reading in the L2 may not be surprising. 

Yet, research has shown that bilinguals experience co-activation of the L2 while reading  reading sentences in the stronger L1, though the number of studies is much more limited. In addition to presenting sentences in the L2, Schwartz and Kroll (2006) also asked their participants to read sentences in a block of their L1 (Spanish). Likewise, Van Assche et al. (2009) tested participants who read in their native Dutch. Both of these studies find cognate effects---evidence for language nonselectivity. These results provide striking evidence that even in L1 reading, a more practiced skill compared to L2 reading, bilinguals activate both languages in parallel. The finding of parallel activity while reading in either the L2 or in the L1, parallels the research on nonselectivity in out of context tasks. 

While there is a decent amount of evidence for parallal activation regardless of whether participants read sentences in the L2 or in the L1, there has been considerably less research that systematically investigates the role of the specific languages in the degree of parallel activation in context. The bilinguals that have been tested thus far have spoken Dutch and English (Duyck et al, Van Assche et al 2009,2010), Spanish and English (Schwartz and Kroll), or French and English (Libben and Titone, Chambers and Cooke). In the studies that have examined processing in the L2, the L2 has most often been English (Duyck et al, Van Assche et al 2010, Libben and Titone ), though Chambers and Cooke studied French as the L2. 

One might argue that this is a confound, however Chambers and Cooke find converging results with French as the L2. Because English and French are from different language families (Germanic and Romance) and because results of out of context studies have been replicated across a variety of different language pairs, concearns about confounds due to the language pairs should be ameiliarated.

Again, only two studies have examined processing in the L1. However, one study has been in Dutch (Van Assche et al. 2009) and another has been in English (Schwartz and Kroll, 2006). 


However, again results from out of context tasks suggest that the specific languages should not play a very large role; parallel activation should occur regardless of the languages.

 
Language nonselectivity out of context has been studied across a wide variety of language pairs. 






While parallel activation out of context may be more observable when the less dominant L2 is in use, it still persists when the L1 is in use.  If it is true that nonselectivity exists in sentence context, then parallel activation should be demonstrable whether it is measured in the L1 or in the L2. 

 participants reliably activate the L1 while reading in their L2. This pattern is intuitive because a more dominant system (L1) is influencing a weaker system (L2). A particularly strong demonstration of language nonselectivity is to show parallel activation of the L2 while processing sentences in the L1. One might imagine that because the use of the L1 is more practiced (i.e., it occurs faster, it is used more often, etc.), recogntion of words may not occur in parallel. However, a subset of the studies described above show that the L1 is permeable to influence from the L2.

A critique of the research supporting nonselectivity is that a majority of the evidence comes from cognates, which arguably have a special representation in the lexicon. Thus the argument can be made that evidence for parallel activation of two languages is really evidence for a special representation of cognates.

%%%% Continuous function of orthographic overlap
Some researchers argue that cognates are specially represented in the mental lexicon. If this is true, then it sheds doubt on the usefulness of the cognate effect as a measure of cross-language parallel activation. The cognate effect may result from a special representation. This potential confound can be addressed by investigating the role of cross-language overlap in word recognition. If the cognate effect is driven by special representation, then word recognition times should not differ as a function of the degree of overlap because all cognates are special. In contrast, if the cognate effect is really a marker of parallel activation, then we should see graded effects that are dependant on the amount of cross-language overlap.  

A subset of the studies discussed above show evidence that the size of the cognate effect [[or is it the reaction times in general]]  depends on the degree of cross-language overlap. Cross-language overlap can be operationalized in terms of orthographic overlap and phonological overlap. For example, hotel and ship are both cognates with Dutch. Hotel, however, has complete orthographic overlap with Dutch whereas ship is written schip in Dutch. Similarly, words that are written the same across languages often differ in their pronunciation (phonological overlap) or in their meaning (i.e., homographs). The typical finding has been that words with less cross-language overlap tend to elicit slower reaction times while words with greater overlap are recognized faster. This justifies the use cognate effect  as a marker of parallel activation. 

Duyck et al. (2007) conducted a discrete analysis using their eye-tracking data  to show that cross-language overlap affects the presence of the cognate effect. In their materials, they included cognates and noncognate words, and the cognate items were further split into identical cognates (e.g., hotel) and nonidentical cognates (e.g., ship-schip). Duyck et al. found that identical cognates were fixated on for a shorter amount of time compared to matched noncognate control words. However, this difference was not significant for nonidentical cognates. This finding provides preliminary evidence for the idea that not all cognates are created equal. Cognates with complete orthographic overlap elicit a greater cognate effect, while the nonidentical cognates elicit a smaller effect that was apprantly not detectable by Duyck et al.'s analysis.  

An alternative explanation for this lack of cognate effect for nonidentical cognates exists. The sentences in this experiment may have been providing lexical restrictions, albeit weak ones, to language of the sentence. The effect of these restrictions in negating parallel activity only emerged when parallel activation was weaker because of incomplete orthographic overlap. This alternate explanation is complementary to the hypothesis that incomplete orthographic overlap alone is the determinant of the lack of cognate effect. However, it is important to disentangle the role, if any, that the sentence context plays from the role that orthographic overlap plays in parallel activation of two languages. 

Duyck et al. (2007) conducted two control studies (out of context lexical decision and in context lexical decision) to provide support the idea that the lack of a cognate effect for nonidentical cognates is due to orthographic overlap  and not to sentential context. They showed that LDT reaction times to cognate words (in and out of context) were influenced by the degree of orthographic overlap: a decrease in orthographic overlap resulted in greater reaction times. This correlation also held for reading times in the eye-tracking experiment. Yet, in both LDTs there was still a significant cognate effect for the nonidentical cognates where there was none in the eye-tracking experiment. The presence of a cognate effect for nonidentical cognates sentential LDT suggests that the nonidentical cognates, though they lacked complete orthographic overlap, were sufficiently able to elicit parallel activation of both languages in the context of a sentence. 

The presence of parallel activation for nonidentical cognates in the sentential lexical decision task leaves open the possibility that the eye-movement record was unable to detect weak parallel activation despite the common assumption that eye-tracking is a more sensitive measure than behavioral tasks. This could occur, counterintuitively, because the method is very sensitive, providing very fast reaction times (on the order of 200ms in Duyck et al., 2007) in comparison to LDT reaction times (around 700ms) for example. Such small effects may be difficult to detect with very fast reaction times and instead may require a measure that magnifies the differences, something to be kept in mind when designing a study that might result in small effects. Overall, the results of the series of studies conducted by Duyck et al. suggest that a sentence context alone is not sufficient to override parallel activation of the L1 when reading in the L2.

%The authors show that the degree of parallel activation is a function of cross-language overlap. Cognates that have greater orthographic overlap across languages tend to be read faster. Duyck et al. conduct a  discrete analysis by comparing nonidentical cognates to identical cognates and show that only identical cognates provide evidence for parallel activation. This finding is honed by Van Assche et al. who conduct a regression analysis to show that this effect is actually continuous: the cognate effect decreases continually as a function of orthographic overlap.

While the results of these control studies suggest that orthographic overlap influences the time to identify a word, a discrete difference between identical and nonidentical cognates does not defeat the theory that cognates have a special representation. One could argue that while identical cognates are specially represented, nonidentical cognates are not cognates are not, explaining why Duyck et al. (2007) did not see facilitation for nonidentical cognates. A more fine grained analysis can adjudicate whether cognates are somehow special.

Van Assche et al. (2009) and Van Assche et al. (2010) found continuous   effects of cross-lingual overlap on reading times in the L2 and in the L2. Where Ducyk et al. (2007) categorized cognates into discrete groups (identical and nonidentical), Van Assche et al. (2009) calculated the orthographic overlap of each target word and its translation according to the method by Van Orden (1987). This method provides a numerical measure of overlap between two words by taking into account such factors as number of letters, number of paired adjacent letters, whether the words share the same first or last letter, etc. This measure of overlap was then entered as a predictor into linear mixed-effects regression models for each of the eye-tracking measures of interest (first fixation duration, gaze-duration, and regression-path duration). While overall model fits were not included in the results, the authors found a significant effect of orthographic overlap on reading times. The time to read a word was inversely proportional to the degree of overlap; as overlap decreased reading times increased. This shows that readers utilized this information during word recognition. 

Van Assche et al. (2010) further show that the degree of phonological overlap is also important for word recognition and can be used as another method to measure cognate status continuously. In addition to entering orthographic overlap into their linear model, they also included phonological overlap. Phonological overlap was obtained by asking a group participants, independent from their main experiments, to rate how similarly a word and its translation sounded. When phonological similarity was entered into the regression model, the authors found an inversely proportional relation to reaction times, similar to that of orthographic overlap.

The results of Ducyk et al. (2007), Van Assche (2009), Van Assche (2010) show that cross-lingual overlap influences how bilinguals recognize words in sentence context. As words across two language increase in the degree of overlap, the speed at which they are recognized increases. This finding is important because it shows that cognate status per se is not what influences recognition. Recognition is influenced by the degree of overlap between a word and its translation equivalent. Therefore it is correct to assume that the cognate effect is a measure of parallel activation of the bilingual's two languages not as proof of a special representation of cognates.

\subsubsection{wrap up}
Taken together, the results of these in context studies all show that language nonselectivity cannot be overcome by a sentence context alone. Words become activated in both languages according to their degree of overlap across languages. The paralell activation is  so pervasive that it persists while bilinguals read sentences in their  stronger native language, not only during L2 reading.  Yet it would seem that at some point, a bilingual should be able to ``choose'' a language. After all, bilinguals can  successfully make lexical decisions in a sentence context, nor do they have a problem assembling the meaning of a sentence. Given the pervasiveness of nonselectivity in sentence context, a second goal of in context  word recognition research has been to explore whether there are any linguistic factors that might allow bilinguals to make a language selection.

\subsection{The role of semantic constraints}
While all studies converge on the finding that  word recognition is nonselective within a sentence context, they diverge on whether recognition may become selective under certain conditions, namely following a highly constrained semantic context.  A subset of the studies examined above illustrates that patterns of word recognition following a highly biased context is consistent with the predictions made by a language-selective access account. In highly constrained  contexts, cognate facilitation is eliminated (Schwartz and Kroll, 2006, Chambers and Cook, 2009, Van Hell and de Groot, 2008, Libben and Titone 2009). However, there is disagreement in the literature as to whether the locus of the effect actually stems from language-selectivity, or whether it is the result of another factor, such as a confounding of the methods or a floor effect caused by the speeded processing due to semantic constraints. Furthermore, not all studies have found an effect of semantic constraints (e.g., Van Assche et al, 2010).

The semantic constraint effect has been replicated with a variety of methods including lexical decision and translation (Van Hell and de Groot, 2008), and eye-tracking during reading and with the Visual World Paradigm (Libben and Titone, 2009; Chambers and Cooke, 2009). The eye-tracking record shows that when semantic constraints have an effect, it occurs during later stages in processing (Libben and Titone 2009). Because the effect of semantic constraints can be seen with a variety of methods, from behavioral to eye-tracking, the effect is likely not due to methods. However, the debate about whether speeded processing creates a floor effect is still open. The current study will approach the question of whether word recognition can become selective by investigating whether another sentential factor, syntactic constraints, can eliminate or reduce parallel activation. 

Schwartz and Kroll (2006) demonstrate that a sentence which is highly constrained in its interpretation appears to eliminate the cognate effect. Schwartz and Kroll varied the semantic constraint of their sentences such that half of the sentences contained a highly constrained semantic biases (i.e., high constraint) while the other half did not (i.e., low constraint). Constraintedness was determined by an offline sentence completion task. The target word of each sentence was removed along with any material following the target. A set of participants who did not complete the main experiment  asked to complete each sentence. If the participants were highly likely to complete a sentence with the actual target word, the sentence was rated as high constraint. If a variety of words followed a sentence, it was rated as low constraint. 

Schwartz and Kroll (2006) found cognate facilitation only in the low constraint sentences. In sentences with a high semantic constraint, the authors found that all target words were named faster. This signifies that the biased context helped participants to access the upcoming target word. Importantly, the authors failed to find a cognate effect for target words following a high semantic constraint.  The cognate facilitation effect in low constraint sentences signifies that a sentence context alone is not enough to constrain word recognition to one language. However, additional semantic cues may provide lexical restrictions to language, resulting in a modulation the cognate effect. 

The finding of apparent selective access in high constraint sentences has been criticized on two grounds. First,  the elimination of the cognate effect may be the result of the method that Schwartz and Kroll used to investigate word recognition (i.e., word naming). Second, the elimination of the cognate effect in high constraint sentences may be the result of the facilitation that occurs following a biased semantic context. That is, because high constraint sentences speed lexical access, in these sentences word recognition may reach a floor and thus cognate facilitation may no longer be witnessed. Each critique warrants further explorations, and with the use of converging methodologies, they can begin to be addressed. 

\subsubsection{Methods}
The semantic constraint effect cannot be due to the methods used by Schwartz and Kroll (2006). The effect has been replicated using other behavioral methods as well as with eye-tracking methods (e.g., Van Hell and de Groot; Libben and Titone; Chambers and Cooke, 2009). The eye-tracking record during reading suggests that the effect of semantic constraints occurs during later stages of word recognition, meaning that word recognition is initially nonselective. While converging methods confirm that semantic constraints eliminate the cognate effect, they cannot confirm that this modulation is due to language-selective access. The reduction in the cognate effect may be due to speeded access of semantic constraints creating a floor effect that over-shadows an effect of cross-language overlap. 

Van Hell and de Groot (2008) show that semantic constraints modulate the cognate effect during word recognition while bilinguals read sentences. Like Schwartz and Kroll (2006), Van hell and de Groot included sentences that had a high semantic constraint in their materials. Recall that they asked one group of participants to make  a lexical decision on the target words while translated the targets and that translation and lexical decisions were faster for cognates when the sentences were low semantic constraint. Crucially, when targets followed a high semantic constraint (which again facilitated reaction times to identify targets), the difference between cognates and controls was reduced, while in lexical decision the difference was eliminated. The degree to which both languages were activate was greatly decreased, in line with the findings of Schwartz and Kroll. Because the constraint effect was replicated with a different set of behavioral methods, it signifies that the findings of Schwartz and Kroll did not depend on the word naming method they used. 

Chambers and Cooke (2009) found an effect of semantic constraints during auditory word recognition while participants viewed a scene containing pictures from the sentences. In low constraint sentences, participants fixated both at the target picture and the distractor picture, which was a homophone in the unintended languages. However following sentences that contained a high semantic constraint, distractor fixations were greatly decreased, providing further evidence that semantic constraints modulated activation of the unintended language. 

Behavioral tasks, such as lexical decision, translation, and word naming, rely on the response made by a participant to a stimulus. Responses  tell us about the later stages of word recognition---the end result of the process. Information about the  earliest stages of the process is potentially lost. Thus, it is conceivable that in high constraint sentences, word recognition  proceeds in an initially nonselective manner and then becomes selective downstream. Such a theory is predicted by the BIA+ model, and if it were true, behavioral methods such as those used by Schwartz and Kroll (2006) and Van Hell and de Groot (2008) might fail to capture evidence for this initial nonselectivity.  While the visual world paradigm used by Chambers and Cooke provides a good temporal measure of the word candidates participants are considering, the analysis they conducted was not fine-grained enough tell whether the semantic constraint effect began initially or later on. Analyses of fixations during sentence reading are sensitive enough to adjudicate between early and late processing differences.  
 
Libben and Titone (2009) provide striking data that suggest word recognition in high constraint sentences proceeds in an initially nonselective manner but becomes  selective over time as the meaning of the sentence is integrated. They show that in high constraint sentences cross-language facilitation and inhibition were only present in early measures of reading (e.g., first-fixation duration). Later measures (e.g., regression path) showed no significant differences between language ambiguous stimuli and control words. This result highlights the fact that the word recognition system is profoundly nonselective at early stages of processing, but that semantic constraints can allow the system to become selective over time. It also further shows that the semantic constraint effect is not unique to behavioral methods, such as word naming. 

Findings from a variety of methods provide evidence that semantic constraints can eliminate activation form the unintended language. This finding is not method specific, though there are differences in terms of the methods accounting for when the effect emerges over the time-course of processing. However, the locus of the effect is still in question; the elimination of the cognate effect may be due to the fact that semantic biases increase word recognition speed. This speed increase may create a floor effect that diminishes the detectability of the cross-language activity. 

\subsubsection{Semantic constraints too fast}
Semantic constraints reliably increase the rate at which upcoming words are recognized (cite original stuff, Schwanenflugel and LaCounte I think), and we have seen that semantic constraints decrease or eliminate effects that are known to measure parallel activation (e.g., cognate effect and homograph inhibition). However, the locus of this modulation is in question. It could stem from two possible sources: language selective cause by increased lexical cues dur to the constraint, or,more trivially, a loss of analytical power due to the decrease in reation times due to the semantic constraint. Much of the prior research cannot address this question, except for, possibly, the results of Van Hell and De Groot.  

The idea behind the floor effect theory is that participants become so fast at predictinng an upcoming word, the cognate effect can no longer be measured. Cross-language activation is still present, however the effect size is so small it cannot be measured. This theory can be tested by examining a situation in which there is a semantic constraint but no floor effect. In such a situation, the theory would predict that a cognate effect would be present, and the magnitude of the effect should be no different than in a low constraint sentence. A difference between the size of the cognate effect would indicate that the semantic constraint is in some way affecting the degree of parallel activation.

The translation task of Van Hell and De Groot (2008) provides the situation outlined above. In their experiments, participants translated target words that were embedded in high and low constraint sentences. Because translation is a more difficult task for participants compared to lexical decision or word naming, reaction times are slower (though no statistical analyses were conducted to test this). Because reaction times were no longer at floor, role of semantic constraints can be investigated. Van Hell and De Groot found cognate effects in both high and low constraint sentences during translation. However, counter to the predictions made by the floor effect theory, the cognate effect was smaller in the high constraint condition, indicating that although participants co-activated both languages during the translation, this coactivation was less when the context was biased towards a single interpretation. This modulation is likely due to a decrease in parallel activation and not a floor effect because reaction times in the task was slower overall. 

[[I can't think of how to transition to the next section here.]]

\subsubsection{Failure to find semantic constraint effects}
Not all studies have managed to find evidence for the semantic constraint modulation of the cognate effect. Though, failure to find an effect is not evidence that no effect exists, especially in the presence of other positive evidence. It may be that semantic constraints do not always allow for words to be selectively accessed from the lexicon. 

Van Assche et al. (2010) failed to find evidence that semantic constraints modulate the cognate effect, despite the inclusion of  sentences that were rated as being very high constraint compared to previous studies. Instead, their Dutch-English bilinguals read cognate words significantly faster than noncognate words in both low and high constraint sentences; the magnitude did not change. According to Van Assche et al. ``this contrasts with Libben and Titone (2009) who found cognate facilitation only in early comprehension measures... Instead, our results suggest that the influence of such factors on the language-selectivity of lexical access is rather limited...'' This result also contrasts with Schwartz and Kroll, Van Hell and De Groot, and Chambers and Cooke. As we know from statistics, lack of a significant effect is not evidence that there is no effect. Instead there are likely many factors which the authors did not consider that likely result in the lack of the effect.  

The main reason that Van Assche et al (2010) failed to find an effect was likely because of the sizes of their cognate effects. For some of the measures, the effects were sub-10ms. These effects are so small the number of participants they recruited is much higher than in previous studies (e.g., 60 participants). A sub-10ms effect, in  addition to potentially not being a meaningful difference, leaves less room to find an interaction, and even if there were an interaction, it would be very difficult to detect statistically. 

Van Assche et al. chose to use nonidentical cognates as a tool to measure of parallel activation. However as we saw earlier, nonidentical cognates tend to elicit less parallel activation (and perhaps none at all) because of their lack in cross-language overlap. In this light, it is not surprising that Van Assche et al. struggled to find a cognate effect. 

This suggests that decreasing parallel activation is not always as easy as including a high semantic constraint to decrease parallel activation. In the Van Assche et al. study there were likely many other factors working against the semantic constraints.

%``Still, the fact that the present eye-tracking results diverge from these [earlier] studies remains... This illustrates the importance of using the more sensitive eye-tracking technique, like the experiments reported in the paper.'' 





(Van Assche, Drieghe, Duyck, Welvaert, \& Hartsuiker, 2010). Current study did not replicate previous studies (Schwartz200; e.g., van Hell \& de Groot, 2008; Libben \& Titone, 2009), offer a weak argument. Failure to reject a null hypothesis (semantic constraint interaction) is not proof of it. If eye-tracking is very sensitive, one would think the effects would emerge. Cognate effect sizes were very small (note the 60 subjects to and significance). Effect size of the interaction would be even smaller.

We know that when the more proficient language is inactive, effects of parallel activation tend to be larger (Jared2001; Marian2003; van Hell2002) Here, the more proficient language and dominant language of the environment was inactive, so cognate effects should have been large In this study, they made use of exclusively nonidentical cognates which, in a previous study, were shown not to have an effect during eye-tracking (Duyck et al., 2007) What does a sub 10ms effect of eye-movements mean? What if they had used identical cognates? Larger effect sizes and more to work with for semantic constraint manipulation? Are these participants more proficient dominant in English than they let on?

Baten et al. (2010) study. Role of word class overlap in modulating parallel activation. They call it semantics, could it be syntax? 

\section{The present investigation}

